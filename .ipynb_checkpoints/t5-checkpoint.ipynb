{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part1文本表示方法\n",
    "### 词向量\n",
    "word2vec核心思想：对上下文环境里的词进行预测。\n",
    "对于每条输入文本，选取一个上下文窗口和一个中心词，并基于这个中心词去预测窗口里其他词出现的概率。是一种高效的在线学习算法（online learning）。\n",
    "对应算法：\n",
    "SG：skip-grams通过input words预测上下文\n",
    "CBOW:Continuous bag of Words:给定上下文，预测目标单词 \n",
    "### SG原理和网络结构\n",
    "Word2Vec模型实际上分为了两个部分，第一部分为建立模型，第二部分是通过模型获取嵌入词向量。\n",
    "先基于训练数据构建一个神经网络，当这个模型训练好以后，我们并不会用这个训练好的模型处理新的任务，我们真正需要的是这个模型通过训练数据所学得的参数，,这些权重在Word2Vec中实际上就是我们试图去学习的“word vectors”。\n",
    "#### SG过程\n",
    "1.首先我们选句子中间的一个词作为我们的输入词，例如我们选取“dog”作为input word；\n",
    "2.有了input word以后，我们再定义一个叫做skip_window的参数，它代表着我们从当前input word的一侧（左边或右边）选取词的数量。如果我们设置skip_window=2，那么我们最终获得窗口中的词（包括input word在内）就是['The', 'dog'，'barked', 'at']。skip_window=2代表着选取左input word左侧2个词和右侧2个词进入我们的窗口，所以整个窗口大小span=2x2=4。另一个参数叫num_skips，它代表着我们从整个窗口中选取多少个不同的词作为我们的output word，当skip_window=2，num_skips=2时，我们将会得到两组 (input word, output word) 形式的训练数据，即 ('dog', 'barked')，('dog', 'the')。\n",
    "3.神经网络基于这些训练数据将会输出一个概率分布，这个概率代表着我们的词典中的每个词作为input word的output word的可能性。这句话有点绕，我们来看个例子。第二步中我们在设置skip_window和num_skips=2的情况下获得了两组训练数据。假如我们先拿一组数据 ('dog', 'barked') 来训练神经网络，那么模型通过学习这个训练样本，会告诉我们词汇表中每个单词当'dog'作为input word时，其作为output word的可能性。也就是说模型的输出概率代表着到我们词典中每个词有多大可能性跟input word同时出现。\n",
    "\n",
    "我们将通过给神经网络输入文本中成对的单词来训练它完成上面所说的概率计算。下面的图中给出了一些我们训练样本的例子。我们选定句子“The quick brown fox jumps over lazy dog”，设定我们的窗口大小为2（window_size=2），也就是说我们仅选输入词前后各两个词和输入词进行组合。\n",
    "我们将通过给神经网络输入文本中成对的单词来训练它完成上面所说的概率计算。下面的图中给出了一些我们训练样本的例子。我们选定句子“The quick brown fox jumps over lazy dog”，设定我们的窗口大小为2（window_size=2），也就是说我们仅选输入词前后各两个词和输入词进行组合。 \n",
    "PS：input word和output word都会被我们进行one-hot编码。仔细想一下，我们的输入被one-hot编码以后大多数维度上都是0（实际上仅有一个位置为1），所以这个向量相当稀疏，那么会造成什么结果呢。如果我们将一个1 x 10000的向量和10000 x 300的矩阵相乘，它会消耗相当大的计算资源，为了高效计算，它仅仅会选择矩阵中对应的向量中维度值为1的索引行：\n",
    "#### SG训练\n",
    "Word2Vec模型是一个超级大的神经网络（权重矩阵规模非常大）。例如：我们拥有10000个单词的词汇表，我们如果想嵌入300维的词向量，那么我们的输入-隐层权重矩阵和隐层-输出层的权重矩阵都会有 10000 x 300 = 300万个权重，在如此庞大的神经网络中进行梯度下降是相当慢的。更糟糕的是，你需要大量的训练数据来调整这些权重并且避免过拟合。百万数量级的权重矩阵和亿万数量级的训练样本意味着训练这个模型将会是个灾难。\n",
    "解决方案：\n",
    "将常见的单词组合（word pairs）或者词组作为单个“words”来处理；\n",
    "对高频次单词进行抽样来减少训练样本的个数；\n",
    "对优化目标采用“negative sampling”方法，这样每个训练样本的训练只会更新一小部分的模型权重，从而降低计算负担。\n",
    "https://github.com/datawhalechina/team-learning-nlp/blob/master/NewsTextClassification/Task5%20%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB2.md\n",
    "#### TextCNN\n",
    "TextCNN利用CNN（卷积神经网络）进行文本特征抽取，不同大小的卷积核分别抽取n-gram特征，卷积计算出的特征图经过MaxPooling保留最大的特征值，然后将拼接成一个向量作为文本的表示。\n",
    "#### TextRNN\n",
    "TextRNN利用RNN（循环神经网络）进行文本特征抽取，由于文本本身是一种序列，而LSTM天然适合建模序列数据。TextRNN将句子中每个词的词向量依次输入到双向双层LSTM，分别将两个方向最后一个有效位置的隐藏层拼接成一个向量作为文本的表示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part2:基于TextCNN、TextRNN的文本表示\n",
    "### TextCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型搭建\n",
    "self.filter_sizes = [2, 3, 4]  # n-gram window\n",
    "self.out_channel = 100\n",
    "self.convs = nn.ModuleList([nn.Conv2d(1, self.out_channel, (filter_size, input_size), bias=True) for filter_size in self.filter_sizes])\n",
    "#前向传播\n",
    "pooled_outputs = []\n",
    "for i in range(len(self.filter_sizes)):\n",
    "    filter_height = sent_len - self.filter_sizes[i] + 1\n",
    "    conv = self.convs[i](batch_embed)\n",
    "    hidden = F.relu(conv)  # sen_num x out_channel x filter_height x 1\n",
    "\n",
    "    mp = nn.MaxPool2d((filter_height, 1))  # (filter_height, filter_width)\n",
    "    # sen_num x out_channel x 1 x 1 -> sen_num x out_channel\n",
    "    pooled = mp(hidden).reshape(sen_num, self.out_channel)\n",
    "    \n",
    "    pooled_outputs.append(pooled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型搭建\n",
    "input_size = config.word_dims\n",
    "\n",
    "self.word_lstm = LSTM(\n",
    "    input_size=input_size,\n",
    "    hidden_size=config.word_hidden_size,\n",
    "    num_layers=config.word_num_layers,\n",
    "    batch_first=True,\n",
    "    bidirectional=True,\n",
    "    dropout_in=config.dropout_input,\n",
    "    dropout_out=config.dropout_hidden,\n",
    ")\n",
    "#前向传播\n",
    "hiddens, _ = self.word_lstm(batch_embed, batch_masks)  # sent_len x sen_num x hidden*2\n",
    "hiddens.transpose_(1, 0)  # sen_num x sent_len x hidden*2\n",
    "\n",
    "if self.training:\n",
    "    hiddens = drop_sequence_sharedmask(hiddens, self.dropout_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x281f76de170>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)-15s %(levelname)s: %(message)s')\n",
    "\n",
    "# set seed \n",
    "seed = 666\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-29 14:45:15,175 INFO: Fold lens [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]\n"
     ]
    }
   ],
   "source": [
    "# split data to 10 fold\n",
    "fold_num = 10\n",
    "data_file = 'train_set.csv'\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def all_data2fold(fold_num, num=10000):\n",
    "    fold_data = []\n",
    "    f = pd.read_csv(data_file, sep='\\t', encoding='UTF-8')\n",
    "    texts = f['text'].tolist()[:num]\n",
    "    labels = f['label'].tolist()[:num]\n",
    "\n",
    "    total = len(labels)\n",
    "\n",
    "    index = list(range(total))\n",
    "    np.random.shuffle(index)\n",
    "\n",
    "    all_texts = []\n",
    "    all_labels = []\n",
    "    for i in index:\n",
    "        all_texts.append(texts[i])\n",
    "        all_labels.append(labels[i])\n",
    "\n",
    "    label2id = {}\n",
    "    for i in range(total):\n",
    "        label = str(all_labels[i])\n",
    "        if label not in label2id:\n",
    "            label2id[label] = [i]\n",
    "        else:\n",
    "            label2id[label].append(i)\n",
    "\n",
    "    all_index = [[] for _ in range(fold_num)]\n",
    "    for label, data in label2id.items():\n",
    "        # print(label, len(data))\n",
    "        batch_size = int(len(data) / fold_num)\n",
    "        other = len(data) - batch_size * fold_num\n",
    "        for i in range(fold_num):\n",
    "            cur_batch_size = batch_size + 1 if i < other else batch_size\n",
    "            # print(cur_batch_size)\n",
    "            batch_data = [data[i * batch_size + b] for b in range(cur_batch_size)]\n",
    "            all_index[i].extend(batch_data)\n",
    "\n",
    "    batch_size = int(total / fold_num)\n",
    "    other_texts = []\n",
    "    other_labels = []\n",
    "    other_num = 0\n",
    "    start = 0\n",
    "    for fold in range(fold_num):\n",
    "        num = len(all_index[fold])\n",
    "        texts = [all_texts[i] for i in all_index[fold]]\n",
    "        labels = [all_labels[i] for i in all_index[fold]]\n",
    "\n",
    "        if num > batch_size:\n",
    "            fold_texts = texts[:batch_size]\n",
    "            other_texts.extend(texts[batch_size:])\n",
    "            fold_labels = labels[:batch_size]\n",
    "            other_labels.extend(labels[batch_size:])\n",
    "            other_num += num - batch_size\n",
    "        elif num < batch_size:\n",
    "            end = start + batch_size - num\n",
    "            fold_texts = texts + other_texts[start: end]\n",
    "            fold_labels = labels + other_labels[start: end]\n",
    "            start = end\n",
    "        else:\n",
    "            fold_texts = texts\n",
    "            fold_labels = labels\n",
    "\n",
    "        assert batch_size == len(fold_labels)\n",
    "\n",
    "        # shuffle\n",
    "        index = list(range(batch_size))\n",
    "        np.random.shuffle(index)\n",
    "\n",
    "        shuffle_fold_texts = []\n",
    "        shuffle_fold_labels = []\n",
    "        for i in index:\n",
    "            shuffle_fold_texts.append(fold_texts[i])\n",
    "            shuffle_fold_labels.append(fold_labels[i])\n",
    "\n",
    "        data = {'label': shuffle_fold_labels, 'text': shuffle_fold_texts}\n",
    "        fold_data.append(data)\n",
    "\n",
    "    logging.info(\"Fold lens %s\", str([len(data['label']) for data in fold_data]))\n",
    "\n",
    "    return fold_data\n",
    "\n",
    "\n",
    "fold_data = all_data2fold(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-29 14:46:28,535 INFO: Total 9000 docs.\n"
     ]
    }
   ],
   "source": [
    "fold_id = 9\n",
    "\n",
    "train_texts = []\n",
    "for i in range(0, fold_id):\n",
    "    data = fold_data[i]\n",
    "    train_texts.extend(data['text'])\n",
    "    \n",
    "logging.info('Total %d docs.' % len(train_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-29 14:46:29,405 INFO: Start training...\n",
      "2020-07-29 14:46:41,285 INFO: 'pattern' package not found; tag filters are not available for English\n",
      "2020-07-29 14:46:41,863 INFO: collecting all words and their counts\n",
      "2020-07-29 14:46:41,863 INFO: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-07-29 14:46:43,037 INFO: collected 5295 word types from a corpus of 8191447 raw words and 9000 sentences\n",
      "2020-07-29 14:46:43,037 INFO: Loading a fresh vocabulary\n",
      "2020-07-29 14:46:43,132 INFO: effective_min_count=5 retains 4335 unique words (81% of original 5295, drops 960)\n",
      "2020-07-29 14:46:43,135 INFO: effective_min_count=5 leaves 8189498 word corpus (99% of original 8191447, drops 1949)\n",
      "2020-07-29 14:46:43,255 INFO: deleting the raw counts dictionary of 5295 items\n",
      "2020-07-29 14:46:43,255 INFO: sample=0.001 downsamples 61 most-common words\n",
      "2020-07-29 14:46:43,255 INFO: downsampling leaves estimated 7070438 word corpus (86.3% of prior 8189498)\n",
      "2020-07-29 14:46:43,272 INFO: estimated required memory for 4335 words and 100 dimensions: 5635500 bytes\n",
      "2020-07-29 14:46:43,272 INFO: resetting layer weights\n",
      "2020-07-29 14:46:44,005 INFO: training model with 8 workers on 4335 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-07-29 14:46:45,019 INFO: EPOCH 1 - PROGRESS: at 28.97% examples, 2029710 words/s, in_qsize 14, out_qsize 1\n",
      "2020-07-29 14:46:46,015 INFO: EPOCH 1 - PROGRESS: at 66.89% examples, 2342771 words/s, in_qsize 15, out_qsize 0\n",
      "2020-07-29 14:46:46,920 INFO: worker thread finished; awaiting finish of 7 more threads\n",
      "2020-07-29 14:46:46,925 INFO: worker thread finished; awaiting finish of 6 more threads\n",
      "2020-07-29 14:46:46,925 INFO: worker thread finished; awaiting finish of 5 more threads\n",
      "2020-07-29 14:46:46,925 INFO: worker thread finished; awaiting finish of 4 more threads\n",
      "2020-07-29 14:46:46,925 INFO: worker thread finished; awaiting finish of 3 more threads\n",
      "2020-07-29 14:46:46,930 INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-29 14:46:46,935 INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-29 14:46:46,935 INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-29 14:46:46,935 INFO: EPOCH - 1 : training on 8191447 raw words (7022624 effective words) took 2.9s, 2400102 effective words/s\n",
      "2020-07-29 14:46:48,155 INFO: EPOCH 2 - PROGRESS: at 35.02% examples, 2462313 words/s, in_qsize 15, out_qsize 0\n",
      "2020-07-29 14:46:49,155 INFO: EPOCH 2 - PROGRESS: at 72.27% examples, 2544266 words/s, in_qsize 15, out_qsize 0\n",
      "2020-07-29 14:46:49,900 INFO: worker thread finished; awaiting finish of 7 more threads\n",
      "2020-07-29 14:46:49,900 INFO: worker thread finished; awaiting finish of 6 more threads\n",
      "2020-07-29 14:46:49,900 INFO: worker thread finished; awaiting finish of 5 more threads\n",
      "2020-07-29 14:46:49,905 INFO: worker thread finished; awaiting finish of 4 more threads\n",
      "2020-07-29 14:46:49,905 INFO: worker thread finished; awaiting finish of 3 more threads\n",
      "2020-07-29 14:46:49,905 INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-29 14:46:49,915 INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-29 14:46:49,915 INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-29 14:46:49,915 INFO: EPOCH - 2 : training on 8191447 raw words (7021998 effective words) took 2.8s, 2541583 effective words/s\n",
      "2020-07-29 14:46:50,915 INFO: EPOCH 3 - PROGRESS: at 34.54% examples, 2430107 words/s, in_qsize 14, out_qsize 1\n",
      "2020-07-29 14:46:51,925 INFO: EPOCH 3 - PROGRESS: at 68.97% examples, 2425967 words/s, in_qsize 15, out_qsize 0\n",
      "2020-07-29 14:46:52,800 INFO: worker thread finished; awaiting finish of 7 more threads\n",
      "2020-07-29 14:46:52,800 INFO: worker thread finished; awaiting finish of 6 more threads\n",
      "2020-07-29 14:46:52,800 INFO: worker thread finished; awaiting finish of 5 more threads\n",
      "2020-07-29 14:46:52,800 INFO: worker thread finished; awaiting finish of 4 more threads\n",
      "2020-07-29 14:46:52,810 INFO: worker thread finished; awaiting finish of 3 more threads\n",
      "2020-07-29 14:46:52,810 INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-29 14:46:52,815 INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-29 14:46:52,815 INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-29 14:46:52,815 INFO: EPOCH - 3 : training on 8191447 raw words (7022727 effective words) took 2.9s, 2424842 effective words/s\n",
      "2020-07-29 14:46:53,830 INFO: EPOCH 4 - PROGRESS: at 33.80% examples, 2359248 words/s, in_qsize 15, out_qsize 0\n",
      "2020-07-29 14:46:54,835 INFO: EPOCH 4 - PROGRESS: at 68.42% examples, 2398537 words/s, in_qsize 15, out_qsize 0\n",
      "2020-07-29 14:46:55,720 INFO: worker thread finished; awaiting finish of 7 more threads\n",
      "2020-07-29 14:46:55,720 INFO: worker thread finished; awaiting finish of 6 more threads\n",
      "2020-07-29 14:46:55,725 INFO: worker thread finished; awaiting finish of 5 more threads\n",
      "2020-07-29 14:46:55,725 INFO: worker thread finished; awaiting finish of 4 more threads\n",
      "2020-07-29 14:46:55,730 INFO: worker thread finished; awaiting finish of 3 more threads\n",
      "2020-07-29 14:46:55,735 INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-29 14:46:55,735 INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-29 14:46:55,735 INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-29 14:46:55,735 INFO: EPOCH - 4 : training on 8191447 raw words (7022648 effective words) took 2.9s, 2406838 effective words/s\n",
      "2020-07-29 14:46:56,745 INFO: EPOCH 5 - PROGRESS: at 33.10% examples, 2315440 words/s, in_qsize 14, out_qsize 1\n",
      "2020-07-29 14:46:57,745 INFO: EPOCH 5 - PROGRESS: at 64.68% examples, 2268709 words/s, in_qsize 13, out_qsize 2\n",
      "2020-07-29 14:46:58,754 INFO: EPOCH 5 - PROGRESS: at 98.43% examples, 2298995 words/s, in_qsize 14, out_qsize 0\n",
      "2020-07-29 14:46:58,782 INFO: worker thread finished; awaiting finish of 7 more threads\n",
      "2020-07-29 14:46:58,785 INFO: worker thread finished; awaiting finish of 6 more threads\n",
      "2020-07-29 14:46:58,785 INFO: worker thread finished; awaiting finish of 5 more threads\n",
      "2020-07-29 14:46:58,785 INFO: worker thread finished; awaiting finish of 4 more threads\n",
      "2020-07-29 14:46:58,785 INFO: worker thread finished; awaiting finish of 3 more threads\n",
      "2020-07-29 14:46:58,795 INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-29 14:46:58,795 INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-29 14:46:58,795 INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-29 14:46:58,805 INFO: EPOCH - 5 : training on 8191447 raw words (7021986 effective words) took 3.1s, 2296114 effective words/s\n",
      "2020-07-29 14:46:58,805 INFO: training on a 40957235 raw words (35111983 effective words) took 14.8s, 2372370 effective words/s\n",
      "2020-07-29 14:46:58,805 INFO: precomputing L2-norms of word weight vectors\n",
      "2020-07-29 14:46:58,805 INFO: saving Word2Vec object under ./word2vec.bin, separately None\n",
      "2020-07-29 14:46:58,805 INFO: not storing attribute vectors_norm\n",
      "2020-07-29 14:46:58,805 INFO: not storing attribute cum_table\n",
      "2020-07-29 14:46:59,035 INFO: saved ./word2vec.bin\n"
     ]
    }
   ],
   "source": [
    "logging.info('Start training...')\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "num_features = 100     # Word vector dimensionality\n",
    "num_workers = 8       # Number of threads to run in parallel\n",
    "\n",
    "train_texts = list(map(lambda x: list(x.split()), train_texts))\n",
    "model = Word2Vec(train_texts, workers=num_workers, size=num_features)\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# save model\n",
    "model.save(\"./word2vec.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-29 14:46:59,045 INFO: loading Word2Vec object from ./word2vec.bin\n",
      "2020-07-29 14:46:59,215 INFO: loading wv recursively from ./word2vec.bin.wv.* with mmap=None\n",
      "2020-07-29 14:46:59,215 INFO: setting ignored attribute vectors_norm to None\n",
      "2020-07-29 14:46:59,215 INFO: loading vocabulary recursively from ./word2vec.bin.vocabulary.* with mmap=None\n",
      "2020-07-29 14:46:59,215 INFO: loading trainables recursively from ./word2vec.bin.trainables.* with mmap=None\n",
      "2020-07-29 14:46:59,215 INFO: setting ignored attribute cum_table to None\n",
      "2020-07-29 14:46:59,215 INFO: loaded ./word2vec.bin\n",
      "2020-07-29 14:46:59,225 INFO: storing 4335x100 projection weights into ./word2vec.txt\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = Word2Vec.load(\"./word2vec.bin\")\n",
    "\n",
    "# convert format\n",
    "model.wv.save_word2vec_format('./word2vec.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
